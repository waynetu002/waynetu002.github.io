---

layout: post
title: deepresearch梳理
category: 技术
tags: MachineLearning
keywords: deepresearch deepsearch

---

* TOC
{:toc}

## 简介

什么是Deep Research? 它是一个深度搜索和调研的Agent，能在5-30分钟内出一份完整的调研报告。注意,它强调"深度搜索+调研"，而非单纯的深度搜索(Deep Search)。

1. 与Deep Search关系，要复现Deep Research，首先要把搜索(Search)做好。给定用户问题，Agent要学会从浏览器或API中搜集相关知识。没有扎实的Search能力，就难以实现Research。

## Deep Search：搜索的本质与难点

[端到端的训练，怎么复现 Deep ReSearch（上） ：先从 Deep Search 做起](https://zhuanlan.zhihu.com/p/1892489650469323191)

多跳搜索和深度研究型搜索的关键在于模仿人的思维链搜索： 
1. 模型首先根据问题进行初步推理，确定基础搜索方向
2. 执行初始搜索，获取第一批信息
3. 基于已获取的信息，进行下一轮推理，确定进一步的搜索方向
4. 执行细化搜索，获取更精准的信息
5. 不断迭代这个"推理→搜索→推理"的循环，直到收集足够信息 在这个过程中，每次搜索都建立在前一次搜索结果的基础上，形成一个连贯的推理链。

传统 RAG 通常是一次性的：在回答问题前进行一次检索，将检索结果放入上下文中。针对动态的、多步骤的检索机制有一些论文
1. Search-o1 是最近比较火的 WebThinker 项目的前身，
    1. 模型在推理过程中可以识别自身知识的不足点，当遇到知识不确定的情况时，模型会自动生成搜索查询，格式为` <|begin_search_query|>搜索词<|end_search_query|>`
    2. 系统检测到这一标记后，暂停模型推理，执行网络搜索
    3. Reason-in-Documents 模块分析搜索结果，提取关键信息
    4. 精炼后的内容被包装在 `<|begin_search_result|>提炼后的检索内容<|end_search_result|>` 中
    5. 模型继续推理，可能进行多轮搜索-精炼循环
    6. 最终生成完整且准确的答案 !
2. DeepRetrieval，用强化学习来训练query改写，奖励函数设计值得一看。
3. Search-R1，Search-R1没有 Search-o1的 Reason-in-Documents模块，检索到的内容是直接完整放到思维链中的。Search-R1针对任务进行了强化学习的训练，使用基于规则的奖励系统，只关注最终结果的正确性
    1. 当接收到用户问题时，模型首先在`<think>`思考标签内进行初步推理分析，识别当前知识储备中的信息缺口。
    2. 若推理过程中发现知识不足，模型将自主触发检索机制，通过`<search>查询内容</search>`格式生成精准搜索指令。
    3. 搜索引擎返回的结果会被结构化封装在`<information>`信息标签内，为后续推理提供可靠的外部知识输入。
    4. 系统支持多轮次检索-推理循环，模型可根据信息完备性动态决定是否发起新一轮检索，直至满足解答需求。
    5. 当判定信息充足时，模型直接通过`<answer>`答案标签输出简洁结论，无需附加解释说明。
4. R1-Searcher，引入了一个两阶段基于结果的强化学习方法，使LLM能够在推理过程中自主调用外部搜索系统
    1. 第一阶段(检索学习训练)：通过检索奖励激励模型学习如何正确调用外部搜索，不关注答案准确性。
    2. 第二阶段(检索结果集成训练)：在确保格式规范的基础上，引入了答案奖励，提升模型有效利用检索信息解决问题的能力

[Deepresearch核心技术：如何通过强化学习增强推理大模型搜索规划及反馈能力？](https://mp.weixin.qq.com/s/2zlrVmiXHJ6bv12YLD68FA) 未细读。


## 能真正留住用户的，一定是"Deep"。

[端到端的训练，怎么复现 Deep ReSearch（中） ：围绕着"Deep"，解构 Jina 项目的实现](https://zhuanlan.zhihu.com/p/1898295379990132543)
为什么要聚焦于"Deep"？因为它是Deep Research的核心价值。当用户愿意等待5-30分钟来获得一份研究报告，他们期待的必然是高质量的内容，如果最终报告无法让用户产生"啊哈"时刻，无法让他们在某些时刻惊叹"这报告真厉害""这报告部分内容写得比我还好""考虑得比我还全面"，那么这款应用很可能注定失败。

什么是真正的"Deep"？
1. 内容篇幅充足。内容过少往往意味着分析不够全面深入，当然也不能过长，过长的内容可能给用户带来阅读压力。
2. 描述具体且有洞察力：比如分析"人工智能对就业市场的影响"时，浅层表达是"AI将替代一些工作岗位"，而深度表达是"AI将重塑金融行业的职业结构，替代初级分析师的数据处理工作，同时创造数据伦理专家和AI-人类协作经理等新岗位，预计到2026年净就业影响为正增长12%"。
3. 引用权威且适当的资料：专业分析报告通常会引用大量权威资料佐证观点。如果你的Deep Research缺乏引用或引用了不可靠来源，将很难让用户信服。

在复现Deep Research时，请始终记住目标是让你的agent变得更"Deep"，围绕这这个目标去构建工作流、调整 prompt 、微调等等等等。

1. 对问题建立对应的评估标准，包含四种核心评估维度：

    1. Definitive（明确性）：判断问题是否需要明确的答案，而非模糊表述。例如："谁发明了微积分？"—需要明确的答案
    2. Freshness（时效性）：判断问题是否需要最新信息。例如："当前美国的利率是多少？"—需要最新经济数据
    3. Plurality（多样性）：判断问题是否需要多个项目或示例。例如："请列出五个最著名的莎士比亚悲剧"—需要列出多个项目
    4. Completeness（完整性）：判断问题是否包含多个需要全面解答的元素。例如："赤壁之战的历史背景、参与者、战略意义及影响是什么？"—需要全面回答多个方面
    这些评估标准背后各自对应不同的Prompt。不同的问题使用不同的评估标准。 这些评估标准将在最终答案生成后使用，即在得到答案之后，系统会通过这些评估标准来检查答案。
2. Jina将深度研究抽象为四个核心动作（action）的不断选择。具体来说，系统会让LLM基于当前的记忆（包括之前的步骤和已获取的知识）通过Prompt来决定采取哪一个核心动作。这四个核心动作分别是：搜索（Search）、阅读（Visit）、思考（Reflect）和回答（Answer）。**每个动作背后都有复杂的实现机制和精细的工程思考**。
    ![](/public/upload/machine/jina_deepsearch.jpg)
    构建了一个循环的任务处理流程，其中维护一个 gaps 问题列表（可以看成是一个动态的执行计划），在一个由模型自主判断的处理闭环中，每个 step，系统从gaps 列表中抽取一个问题，每个 step 由模型自主根据上下文信息，采取几种 action之一。这样一步步循环推进，直到 gaps 列表清空，或达到 token 限制。整个过程中，依靠系统 system message 和 knowledge message 来进行上下文管理。

## 端到端 vs 固定工作流：两者的折中方案

真正端到端形态的，长的像是豆包目前的“深度思考”功能，如下图。豆包的深度思考实现了“边想边搜”的能力。在一条单一的思维链中，模型一边推理一边发起搜索请求，只需要维护一条持续生长的长链即可。

![](/public/upload/machine/doubao_deepsearch.jpg)

目前不少厂商推出的Deep Research功能，其实更偏向于端到端和固定工作流的中间形态。这些系统会先将任务抽象成若干个action，再依据预设的 plan 和上下文记忆，让llm自己决定当前step应该采取的 action。从架构上看，这其实更像是“多轮对话系统”——每个 step 类似于一轮对话，由模型决定下一步要采取的行动。

不过，从工程实践的角度，这种“折中式”方案反而更加可控和高效，原因有以下几点：

1. 提高效率与资源利用率：将任务拆解后，不同模型可以分工协作，各司其职。
    1. 例如：专门针对规划任务训练过的小模型负责规划，推理模型负责处理复杂推理，擅长写作的模型负责总结写作等生成任务。就像一个项目团队，有实习生、初级工程师和高级工程师，要把任务拆分，协同作业，更容易做成一个 big project，而不是把所有活都交给最厉害的人干，**一个人干完当然可以，但再厉害的人也无法面面兼顾，且会效率低下**。
    2. OpenAI 的 CPO Kevin Weil 在近期的一次播客中就提到，好的系统设计更倾向于多模型协同，“All-in-one”并不一定是最优解
2. 适当抽象和任务拆分，适合 scaling：**通过将任务中常见操作抽象为有限的 action**（例如：调用外部工具、读取结果、反思规划、尝试回答等），模型只需根据上下文动态选择合适的 action。这种抽象之后，系统的可扩展性大大提升
    1. 在广度上：只需新增各类 MCP（工具接口），如文献搜索、Google 查询、Markdown 自动生成等，就能不断拓展系统能力。
    2. 在深度上：可以通过强制执行多轮反思、设定 token 使用下限、对回答进行不同维度的评估等策略，确保模型深入思考、避免浮于表面。
这种机制下，我们更推崇的是一种理念：Less Control, More Tools/MCPs（不是 0 控制，而是适度控制 + 工具赋能）。

## 模型怎么训练

现在的主流训练范式，“推理 + 强化学习（RL）”已经跑通了，只要我们能定义好如何在与环境交互时设计合适的奖励机制（当我们学会这个问题怎么定义怎么评估时，其实我们就多多少少知道奖励该怎么设计），用这种范式训练模型后就能让效果直接拔高。然而，问题也随之而来：“Deep Research”这样的复杂应用为例，奖励到底应该如何设计呢？这一点并不简单。举个例子，现在有两个报告：
1. 报告 1 写作非常流畅、逻辑严谨，但引用不够丰富。
2. 报告 2 写作较差，但引用信息丰富且权威。

我们应该给予哪份报告更多的奖励？对于 Deep Research 这样的任务，我们需要更细粒度的评估标准，比如引用是否合理多样、报告中的公式计算是否准确、是否能够挖掘长尾信息等等等等。并不是简单地给报告打一个0到10分的奖励就能解决问题。

OpenAI 的 Deep Research 在他们的 System card 中展示了训练数据和方法。他们的训练数据集涵盖了从有标准答案的“objective auto-gradable tasks with ground truth answers”到“more open-ended tasks with accompanying rubrics for grading”。在训练过程中，他们使用思维链模型作为评分器，将模型的输出与标准答案或评分标准进行对比评分。这意味着，openai的训练数据既包括像简单的多跳问答这种有固定答案的任务，也包括更加开放式的写作任务。字节最近发布的一篇论文《Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback》也讨论了RL 的训练问题。论文指出，基于规则的设计更容易帮助模型学习到 fine-grained 的信息，不容易遭受“reward hacking”。而纯开放式任务则可能被模型“钻空子”，只学会表面技巧，忽视了任务的深层价值，因此只能学习到coarse grained信息。DeepSeek-R1 的最后强化学习阶段，也是通过混合使用基于规则的奖励和开放的奖励模型来进行训练。因此，目前的建议是：最好结合有标准答案的任务（做 rule-based reward）与开放式任务（要专门训一个reward model来奖励），将这两种类型的奖励混合使用。除此之外，其他的训练策略或许还需要社区更多的研究工作。PS: 就是你不要用单一目标来训练模型

目前市面上的 Deep Research 已经具备一定能力，但说实话感觉离直接把报告当成一个专业分析师的报告还有一段距离，工程的优化是有上限的，未来需要更明确的训练方式来提升效果。
